{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: f:\\Projects\\sparkimental\n",
      "HADOOP_HOME ENV var:  f:\\Projects\\sparkimental/windows/winutils/hadoop-3.0.0\n"
     ]
    }
   ],
   "source": [
    "#  Get CWD\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "print(f'CWD: {cwd}')\n",
    "import sys\n",
    "\n",
    "hadoop_home_path = f'{cwd}/windows/winutils/hadoop-3.0.0'\n",
    "os.environ['HADOOP_HOME'] = hadoop_home_path\n",
    "sys.path.append(f'{hadoop_home_path}/bin')\n",
    "\n",
    "print('HADOOP_HOME ENV var: ', os.environ['HADOOP_HOME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f:\\\\AppSSD\\\\WorkTools\\\\Anaconda\\\\envs\\\\sparkimental\\\\lib\\\\site-packages\\\\pyspark'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import findspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.types import *\n",
    "from pprint import pprint, pformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version:  3.1.2\n",
      "Hadoop version: 3.2.0\n"
     ]
    }
   ],
   "source": [
    "# Setup spark environment\n",
    "sc = SparkContext.getOrCreate()\n",
    "print('Spark version: ', sc.version)\n",
    "print(f'Hadoop version: {sc._jvm.org.apache.hadoop.util.VersionInfo.getVersion()}')\n",
    "# print('Spark Config:')\n",
    "# pprint(sc.getConf().getAll())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark web UI link:  http://ProdudePredator.mshome.net:4040\n"
     ]
    }
   ],
   "source": [
    "print('Spark web UI link: ', sc._jsc.sc().uiWebUrl().get())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra import (move later)\n",
    "from pyspark import SparkFiles\n",
    "from pyspark import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add data file from local system to Spark's RDD\n",
    "path = cwd.replace('\\\\', '/').replace('f:/','')\n",
    "full_path = f'file:///{path}/data/animal-crossing.csv'\n",
    "sc.addFile(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sql context\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File path in Spark File system:  C:\\Users\\USER\\AppData\\Local\\Temp\\spark-3d41376e-a71c-4198-a853-ce367ea325ef\\userFiles-e7101d4a-d39d-4f4a-90c8-c4776a2a3b6e\\animal-crossing.csv\n"
     ]
    }
   ],
   "source": [
    "# read file from hdfs/rdd\n",
    "file_path = SparkFiles.get('animal-crossing.csv')\n",
    "print('File path in Spark File system: ', file_path)\n",
    "df = sqlContext.read.csv(path=file_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- grade: integer (nullable = true)\n",
      " |-- publication: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- grade: integer (nullable = true)\n",
      " |-- publication: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Type cast\n",
    "df = df.withColumn('date', df.date.cast(DateType()))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (107, 3)\n",
      "+----------+--------------------+-----+\n",
      "|      date|                text|grade|\n",
      "+----------+--------------------+-----+\n",
      "|2020-03-16|With a game this ...|  100|\n",
      "|2020-03-16|Similar to how Br...|  100|\n",
      "|2020-03-16|Animal Crossing: ...|   93|\n",
      "|2020-03-16|New Horizons has ...|   90|\n",
      "|2020-03-16|It’s a blissfully...|   80|\n",
      "|2020-03-23|Animal Crossing N...|   80|\n",
      "|2020-03-24|Animal Crossing: ...|   98|\n",
      "|2020-03-26|Quotation forthco...|   90|\n",
      "|2020-04-02|If Animal Crossin...|   90|\n",
      "|2020-04-08|It's an exception...|   90|\n",
      "+----------+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop useless column\n",
    "df = df.select(['date', 'text', 'grade'])\n",
    "print(f'Data shape: {df.count(), len(df.columns)}')\n",
    "# Show data samples\n",
    "df.sample(0.1, seed=0).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----------------+\n",
      "|summary|                text|            grade|\n",
      "+-------+--------------------+-----------------+\n",
      "|  count|                 107|              107|\n",
      "|   mean|                null| 90.6355140186916|\n",
      "| stddev|                null|6.114308185868841|\n",
      "|    min|A beautiful, welc...|               70|\n",
      "|    max|“New Horizons mak...|              100|\n",
      "+-------+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Basic stats\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan checking\n",
      "+-----+\n",
      "|grade|\n",
      "+-----+\n",
      "|    0|\n",
      "+-----+\n",
      "\n",
      "+----+\n",
      "|text|\n",
      "+----+\n",
      "|   0|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Nan check\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "print(\"Nan checking\")\n",
    "df.select(\n",
    "    [\n",
    "        count(\n",
    "            when(\n",
    "                isnan(c)\n",
    "                | col(c).isNull()\n",
    "                | (col(c) == \"\")\n",
    "                | col(c).contains(\"None\")\n",
    "                | col(c).contains(\"Null\"),\n",
    "                c,\n",
    "            )\n",
    "        ).alias(c)\n",
    "        for c in [\"grade\"]\n",
    "    ]\n",
    ").show()\n",
    "\n",
    "df.select(\n",
    "    [\n",
    "        count(\n",
    "            when(\n",
    "                col(c).contains(\"None\")\n",
    "                | col(c).contains(\"NULL\")\n",
    "                | (col(c) == \"\")\n",
    "                | col(c).isNull()\n",
    "                | isnan(c),\n",
    "                c,\n",
    "            )\n",
    "        ).alias(c)\n",
    "        for c in [\"text\"]\n",
    "    ]\n",
    ").show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparkimental",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e4315361361e5693679a33b7caab5ef4ac71969d7d5a82cbcec4fbfd006b2427"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

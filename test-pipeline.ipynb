{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Pipeline\n",
    "1. Quick Explore\n",
    "2. Check NULL\n",
    "## NLP Pipeline steps\n",
    "1. Stop word removal\n",
    "2. Casing removal\n",
    "3. Tokenization\n",
    "4. Stemming (or Lemmatization)\n",
    "5. Embedding/Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: f:\\Projects\\sparkimental\n",
      "HADOOP_HOME ENV var:  f:\\Projects\\sparkimental/windows/winutils/hadoop-3.0.0\n"
     ]
    }
   ],
   "source": [
    "#  Get CWD\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "print(f'CWD: {cwd}')\n",
    "import sys\n",
    "\n",
    "hadoop_home_path = f'{cwd}/windows/winutils/hadoop-3.0.0'\n",
    "os.environ['HADOOP_HOME'] = hadoop_home_path\n",
    "sys.path.append(f'{hadoop_home_path}/bin')\n",
    "\n",
    "print('HADOOP_HOME ENV var: ', os.environ['HADOOP_HOME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import findspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.types import *\n",
    "from pprint import pprint, pformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f:\\\\AppSSD\\\\WorkTools\\\\Anaconda\\\\envs\\\\sparkimental\\\\lib\\\\site-packages\\\\pyspark'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find spark to make sure pyspark works (note: make sure only :))\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version:  3.1.2\n",
      "Hadoop version = 3.2.0\n",
      "Spark Config:\n",
      "[('spark.sql.warehouse.dir', 'file:/f:/Projects/sparkimental/spark-warehouse'),\n",
      " ('spark.driver.host', 'ProdudePredator.mshome.net'),\n",
      " ('spark.app.startTime', '1666862767841'),\n",
      " ('spark.app.id', 'local-1666862769943'),\n",
      " ('spark.rdd.compress', 'True'),\n",
      " ('spark.serializer.objectStreamReset', '100'),\n",
      " ('spark.master', 'local[*]'),\n",
      " ('spark.submit.pyFiles', ''),\n",
      " ('spark.executor.id', 'driver'),\n",
      " ('spark.submit.deployMode', 'client'),\n",
      " ('spark.driver.port', '60313'),\n",
      " ('spark.ui.showConsoleProgress', 'true'),\n",
      " ('spark.app.name', 'pyspark-shell')]\n"
     ]
    }
   ],
   "source": [
    "# Setup spark environment\n",
    "sc = SparkContext.getOrCreate()\n",
    "print('Spark version: ', sc.version)\n",
    "print(f'Hadoop version = {sc._jvm.org.apache.hadoop.util.VersionInfo.getVersion()}')\n",
    "print('Spark Config:')\n",
    "pprint(sc.getConf().getAll())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra import (move later)\n",
    "from pyspark import SparkFiles\n",
    "from pyspark import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add data file from local system to Spark's RDD\n",
    "path = cwd.replace('\\\\', '/').replace('f:/','')\n",
    "full_path = f'file:///{path}/data/animal-crossing.csv'\n",
    "sc.addFile(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sql context\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File path in Spark File system:  C:\\Users\\USER\\AppData\\Local\\Temp\\spark-31a43f65-7128-4b5a-8950-5fe3648df162\\userFiles-e4460f48-1d09-4d43-8ce7-f9ea58c4cbbe\\animal-crossing.csv\n"
     ]
    }
   ],
   "source": [
    "# read file from hdfs/rdd\n",
    "file_path = SparkFiles.get('animal-crossing.csv')\n",
    "print('File path in Spark File system: ', file_path)\n",
    "df = sqlContext.read.csv(path=file_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- grade: integer (nullable = true)\n",
      " |-- publication: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- grade: integer (nullable = true)\n",
      " |-- publication: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Type cast\n",
    "df = df.withColumn('date', df.date.cast(DateType()))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (107, 3)\n",
      "+----------+--------------------+-----+\n",
      "|      date|                text|grade|\n",
      "+----------+--------------------+-----+\n",
      "|2020-03-16|With a game this ...|  100|\n",
      "|2020-03-16|Similar to how Br...|  100|\n",
      "|2020-03-16|Animal Crossing: ...|   93|\n",
      "|2020-03-16|New Horizons has ...|   90|\n",
      "|2020-03-16|It’s a blissfully...|   80|\n",
      "|2020-03-23|Animal Crossing N...|   80|\n",
      "|2020-03-24|Animal Crossing: ...|   98|\n",
      "|2020-03-26|Quotation forthco...|   90|\n",
      "|2020-04-02|If Animal Crossin...|   90|\n",
      "|2020-04-08|It's an exception...|   90|\n",
      "+----------+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop useless column\n",
    "df = df.select(['date', 'text', 'grade'])\n",
    "print(f'Data shape: {df.count(), len(df.columns)}')\n",
    "# Show data samples\n",
    "df.sample(0.1, seed=0).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----------------+\n",
      "|summary|                text|            grade|\n",
      "+-------+--------------------+-----------------+\n",
      "|  count|                 107|              107|\n",
      "|   mean|                null| 90.6355140186916|\n",
      "| stddev|                null|6.114308185868841|\n",
      "|    min|A beautiful, welc...|               70|\n",
      "|    max|“New Horizons mak...|              100|\n",
      "+-------+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Basic stats\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan checking\n",
      "+-----+\n",
      "|grade|\n",
      "+-----+\n",
      "|    0|\n",
      "+-----+\n",
      "\n",
      "+----+\n",
      "|text|\n",
      "+----+\n",
      "|   0|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Nan check\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "print(\"Nan checking\")\n",
    "df.select(\n",
    "    [\n",
    "        count(\n",
    "            when(\n",
    "                isnan(c)\n",
    "                | col(c).isNull()\n",
    "                | (col(c) == \"\")\n",
    "                | col(c).contains(\"None\")\n",
    "                | col(c).contains(\"Null\"),\n",
    "                c,\n",
    "            )\n",
    "        ).alias(c)\n",
    "        for c in [\"grade\"]\n",
    "    ]\n",
    ").show()\n",
    "\n",
    "df.select(\n",
    "    [\n",
    "        count(\n",
    "            when(\n",
    "                col(c).contains(\"None\")\n",
    "                | col(c).contains(\"NULL\")\n",
    "                | (col(c) == \"\")\n",
    "                | col(c).isNull()\n",
    "                | isnan(c),\n",
    "                c,\n",
    "            )\n",
    "        ).alias(c)\n",
    "        for c in [\"text\"]\n",
    "    ]\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refs:\n",
    "1. https://github.com/SurajMalpani/NLP-using-Spark/blob/master/nlp-using-pyspark-ml.ipynb\n",
    "2. https://stackoverflow.com/questions/53579444/efficient-text-preprocessing-using-pyspark-clean-tokenize-stopwords-stemming\n",
    "3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```Build steps into pipeline after done```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover, RegexTokenizer, CountVectorizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "df = df.select('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+--------------------------------------------------+\n",
      "|                                              text|                                             words|\n",
      "+--------------------------------------------------+--------------------------------------------------+\n",
      "|Animal Crossing; New Horizons, much like its pr...|[animal, crossing;, new, horizons,, much, like,...|\n",
      "|Know that if you’re overwhelmed with the world,...|[know, that, if, you’re, overwhelmed, with, the...|\n",
      "|With a game this broad and lengthy, there’s mor...|[with, a, game, this, broad, and, lengthy,, the...|\n",
      "|Animal Crossing: New Horizons is everything I h...|[animal, crossing:, new, horizons, is, everythi...|\n",
      "|Above all else, Animal Crossing: New Horizons i...|[above, all, else,, animal, crossing:, new, hor...|\n",
      "+--------------------------------------------------+--------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regex_tokenizer = RegexTokenizer(inputCol='text', outputCol='words', toLowercase=True)\n",
    "raw_words_df = regex_tokenizer.transform(df)\n",
    "raw_words_df.show(5, truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+--------------------------------------------------+\n",
      "|                                             words|                                          filtered|\n",
      "+--------------------------------------------------+--------------------------------------------------+\n",
      "|[animal, crossing;, new, horizons,, much, like,...|[animal, crossing;, new, horizons,, much, like,...|\n",
      "|[know, that, if, you’re, overwhelmed, with, the...|[know, you’re, overwhelmed, world,, stuck, insi...|\n",
      "|[with, a, game, this, broad, and, lengthy,, the...|[game, broad, lengthy,, there’s, discuss, fit, ...|\n",
      "|[animal, crossing:, new, horizons, is, everythi...|[animal, crossing:, new, horizons, everything, ...|\n",
      "|[above, all, else,, animal, crossing:, new, hor...|[else,, animal, crossing:, new, horizons, unbea...|\n",
      "+--------------------------------------------------+--------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stop_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "filtered_df = stop_remover.transform(raw_words_df)\n",
    "filtered_df.select(['words', 'filtered']).show(5, truncate=50)\n",
    "# Note how 'a' is removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+--------------------------------------------------+\n",
      "|                                          filtered|                                           stemmed|\n",
      "+--------------------------------------------------+--------------------------------------------------+\n",
      "|[animal, crossing;, new, horizons,, much, like,...|[anim, crossing;, new, horizons,, much, like, p...|\n",
      "|[know, you’re, overwhelmed, world,, stuck, insi...|[know, you'r, overwhelm, world,, stuck, inside,...|\n",
      "|[game, broad, lengthy,, there’s, discuss, fit, ...|[game, broad, lengthy,, there, discuss, fit, on...|\n",
      "|[animal, crossing:, new, horizons, everything, ...|[anim, crossing:, new, horizon, everyth, hope, ...|\n",
      "|[else,, animal, crossing:, new, horizons, unbea...|[else,, anim, crossing:, new, horizon, unbeat, ...|\n",
      "+--------------------------------------------------+--------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# Why custom class? https://stackoverflow.com/questions/51415784/how-to-add-my-own-function-as-a-custom-stage-in-a-ml-pyspark-pipeline\n",
    "class StemmerTransformer(Transformer):\n",
    "    def __init__(self, inputCol: str, outputCol: str, language: str):\n",
    "        super(StemmerTransformer, self).__init__()\n",
    "        self._stemmer = SnowballStemmer(language=language)\n",
    "        self.udf = udf(\n",
    "            lambda tokens: [\n",
    "                self._stemmer.stem(token) for token in tokens\n",
    "            ], ArrayType(StringType()))\n",
    "        self.inputCol = inputCol\n",
    "        self.outputCol = outputCol\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        df = df.withColumn(self.outputCol, self.udf(self.inputCol))\n",
    "        return df\n",
    "\n",
    "stemmer = StemmerTransformer(inputCol='filtered', outputCol='stemmed', language='english')\n",
    "stemmed_df = stemmer.transform(filtered_df)\n",
    "stemmed_df.select('filtered', 'stemmed').show(5, truncate=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+--------------------------------------------------+\n",
      "|                                           stemmed|                                          embedded|\n",
      "+--------------------------------------------------+--------------------------------------------------+\n",
      "|[anim, crossing;, new, horizons,, much, like, p...|(1369,[0,1,9,17,23,28,58,73,137,194,283,330,639...|\n",
      "|[know, you'r, overwhelm, world,, stuck, inside,...|(1369,[1,16,21,48,67,104,128,135,220,230,312,31...|\n",
      "|[game, broad, lengthy,, there, discuss, fit, on...|(1369,[3,6,9,14,15,16,21,30,31,34,36,45,60,94,9...|\n",
      "|[anim, crossing:, new, horizon, everyth, hope, ...|(1369,[0,1,2,4,5,6,29,32,57,64,70,92,95,126,175...|\n",
      "|[else,, anim, crossing:, new, horizon, unbeat, ...|(1369,[0,1,2,3,4,6,7,14,16,18,31,33,35,44,59,72...|\n",
      "+--------------------------------------------------+--------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(inputCol='stemmed', outputCol='embedded')\n",
    "cv_model = count_vectorizer.fit(stemmed_df)\n",
    "\n",
    "embedded_df = cv_model.transform(stemmed_df)\n",
    "embedded_df.select('stemmed', 'embedded').show(5, truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|               words|            filtered|             stemmed|            embedded|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|Animal Crossing; ...|[animal, crossing...|[animal, crossing...|[anim, crossing;,...|(1369,[0,1,9,17,2...|\n",
      "|Know that if you’...|[know, that, if, ...|[know, you’re, ov...|[know, you'r, ove...|(1369,[1,16,21,48...|\n",
      "|With a game this ...|[with, a, game, t...|[game, broad, len...|[game, broad, len...|(1369,[3,6,9,14,1...|\n",
      "|Animal Crossing: ...|[animal, crossing...|[animal, crossing...|[anim, crossing:,...|(1369,[0,1,2,4,5,...|\n",
      "|Above all else, A...|[above, all, else...|[else,, animal, c...|[else,, anim, cro...|(1369,[0,1,2,3,4,...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stages = [\n",
    "    regex_tokenizer,\n",
    "    stop_remover,\n",
    "    stemmer,\n",
    "    count_vectorizer\n",
    "]\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "preprocess_pipeline = Pipeline(stages=stages)\n",
    "pipeModel = preprocess_pipeline.fit(df)\n",
    "training = pipeModel.transform(df)\n",
    "training.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparkimental",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e4315361361e5693679a33b7caab5ef4ac71969d7d5a82cbcec4fbfd006b2427"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

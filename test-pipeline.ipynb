{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Pipeline\n",
    "1. Quick Explore\n",
    "2. Check NULL\n",
    "## NLP Pipeline steps\n",
    "1. Stop word removal\n",
    "2. Casing removal\n",
    "3. Tokenization\n",
    "4. Stemming (or Lemmatization)\n",
    "5. Embedding/Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: f:\\Projects\\sparkimental\n",
      "HADOOP_HOME ENV var:  f:\\Projects\\sparkimental/windows/winutils/hadoop-3.0.0\n"
     ]
    }
   ],
   "source": [
    "#  Get CWD\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "print(f'CWD: {cwd}')\n",
    "import sys\n",
    "\n",
    "hadoop_home_path = f'{cwd}/windows/winutils/hadoop-3.0.0'\n",
    "os.environ['HADOOP_HOME'] = hadoop_home_path\n",
    "sys.path.append(f'{hadoop_home_path}/bin')\n",
    "\n",
    "print('HADOOP_HOME ENV var: ', os.environ['HADOOP_HOME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import findspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.types import *\n",
    "from pprint import pprint, pformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f:\\\\AppSSD\\\\WorkTools\\\\Anaconda\\\\envs\\\\sparkimental\\\\lib\\\\site-packages\\\\pyspark'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find spark to make sure pyspark works (note: make sure only :))\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version:  3.1.2\n",
      "Hadoop version = 3.2.0\n",
      "Spark Config:\n",
      "[('spark.driver.host', 'ProdudePredator.mshome.net'),\n",
      " ('spark.rdd.compress', 'True'),\n",
      " ('spark.serializer.objectStreamReset', '100'),\n",
      " ('spark.master', 'local[*]'),\n",
      " ('spark.submit.pyFiles', ''),\n",
      " ('spark.executor.id', 'driver'),\n",
      " ('spark.submit.deployMode', 'client'),\n",
      " ('spark.app.startTime', '1666846977289'),\n",
      " ('spark.ui.showConsoleProgress', 'true'),\n",
      " ('spark.app.name', 'pyspark-shell'),\n",
      " ('spark.app.id', 'local-1666846979378'),\n",
      " ('spark.driver.port', '60308')]\n"
     ]
    }
   ],
   "source": [
    "# Setup spark environment\n",
    "sc = SparkContext.getOrCreate()\n",
    "print('Spark version: ', sc.version)\n",
    "print(f'Hadoop version = {sc._jvm.org.apache.hadoop.util.VersionInfo.getVersion()}')\n",
    "print('Spark Config:')\n",
    "pprint(sc.getConf().getAll())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra import (move later)\n",
    "from pyspark import SparkFiles\n",
    "from pyspark import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fullpath: file:///Projects/sparkimental/data/animal-crossing.csv\n"
     ]
    }
   ],
   "source": [
    "# NOTE: this sometimes fail ??\n",
    "path = cwd.replace('\\\\', '/').replace('f:/','')\n",
    "full_path = f'file:///{path}/data/animal-crossing.csv'\n",
    "print(f'fullpath: {full_path}')\n",
    "sc.addFile(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sql context\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File path in Spark File system:  C:\\Users\\USER\\AppData\\Local\\Temp\\spark-fd54d98c-0f37-4835-bd97-df201a34b24f\\userFiles-650c3e5a-9fd5-494d-8745-4fb8b42b7bd0\\animal-crossing.csv\n"
     ]
    }
   ],
   "source": [
    "# read file from hdfs/rdd\n",
    "file_path = SparkFiles.get('animal-crossing.csv')\n",
    "print('File path in Spark File system: ', file_path)\n",
    "df = sqlContext.read.csv(path=file_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- grade: integer (nullable = true)\n",
      " |-- publication: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparkimental",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e4315361361e5693679a33b7caab5ef4ac71969d7d5a82cbcec4fbfd006b2427"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

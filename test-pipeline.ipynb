{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Pipeline\n",
    "1. Quick Explore\n",
    "2. Check NULL\n",
    "## NLP Pipeline steps\n",
    "1. Stop word removal\n",
    "2. Casing removal\n",
    "3. Tokenization\n",
    "4. Stemming (or Lemmatization)\n",
    "5. Embedding/Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: f:\\Projects\\sparkimental\n",
      "HADOOP_HOME ENV var:  f:\\Projects\\sparkimental/windows/winutils/hadoop-3.0.0\n"
     ]
    }
   ],
   "source": [
    "#  Get CWD\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "print(f'CWD: {cwd}')\n",
    "import sys\n",
    "\n",
    "hadoop_home_path = f'{cwd}/windows/winutils/hadoop-3.0.0'\n",
    "os.environ['HADOOP_HOME'] = hadoop_home_path\n",
    "sys.path.append(f'{hadoop_home_path}/bin')\n",
    "\n",
    "print('HADOOP_HOME ENV var: ', os.environ['HADOOP_HOME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import findspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.types import *\n",
    "from pprint import pprint, pformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f:\\\\AppSSD\\\\WorkTools\\\\Anaconda\\\\envs\\\\sparkimental\\\\lib\\\\site-packages\\\\pyspark'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find spark to make sure pyspark works (note: make sure only :))\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version:  3.1.2\n",
      "Hadoop version = 3.2.0\n",
      "Spark Config:\n",
      "[('spark.driver.host', 'ProdudePredator.mshome.net'),\n",
      " ('spark.app.startTime', '1666862767841'),\n",
      " ('spark.app.id', 'local-1666862769943'),\n",
      " ('spark.rdd.compress', 'True'),\n",
      " ('spark.serializer.objectStreamReset', '100'),\n",
      " ('spark.master', 'local[*]'),\n",
      " ('spark.submit.pyFiles', ''),\n",
      " ('spark.executor.id', 'driver'),\n",
      " ('spark.submit.deployMode', 'client'),\n",
      " ('spark.driver.port', '60313'),\n",
      " ('spark.ui.showConsoleProgress', 'true'),\n",
      " ('spark.app.name', 'pyspark-shell')]\n"
     ]
    }
   ],
   "source": [
    "# Setup spark environment\n",
    "sc = SparkContext.getOrCreate()\n",
    "print('Spark version: ', sc.version)\n",
    "print(f'Hadoop version = {sc._jvm.org.apache.hadoop.util.VersionInfo.getVersion()}')\n",
    "print('Spark Config:')\n",
    "pprint(sc.getConf().getAll())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra import (move later)\n",
    "from pyspark import SparkFiles\n",
    "from pyspark import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add data file from local system to Spark's RDD\n",
    "path = cwd.replace('\\\\', '/').replace('f:/','')\n",
    "full_path = f'file:///{path}/data/animal-crossing.csv'\n",
    "sc.addFile(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sql context\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File path in Spark File system:  C:\\Users\\USER\\AppData\\Local\\Temp\\spark-31a43f65-7128-4b5a-8950-5fe3648df162\\userFiles-e4460f48-1d09-4d43-8ce7-f9ea58c4cbbe\\animal-crossing.csv\n"
     ]
    }
   ],
   "source": [
    "# read file from hdfs/rdd\n",
    "file_path = SparkFiles.get('animal-crossing.csv')\n",
    "print('File path in Spark File system: ', file_path)\n",
    "df = sqlContext.read.csv(path=file_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- grade: integer (nullable = true)\n",
      " |-- publication: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- grade: integer (nullable = true)\n",
      " |-- publication: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Type cast\n",
    "df = df.withColumn('date', df.date.cast(DateType()))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (107, 3)\n"
     ]
    }
   ],
   "source": [
    "# Drop useless column\n",
    "df = df.select(['date', 'text', 'grade'])\n",
    "print(f'Data shape: {df.count(), len(df.columns)}')\n",
    "# Show data samples\n",
    "df.sample(0.1, seed=0).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-----------+--------------------+\n",
      "|summary|            grade|publication|                text|\n",
      "+-------+-----------------+-----------+--------------------+\n",
      "|  count|              107|        107|                 107|\n",
      "|   mean| 90.6355140186916|       null|                null|\n",
      "| stddev|6.114308185868841|       null|                null|\n",
      "|    min|               70|   3DJuegos|A beautiful, welc...|\n",
      "|    max|              100|        XGN|“New Horizons mak...|\n",
      "+-------+-----------------+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Basic stats\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan checking\n",
      "+-----+-----------+\n",
      "|grade|publication|\n",
      "+-----+-----------+\n",
      "|    0|          0|\n",
      "+-----+-----------+\n",
      "\n",
      "+----+\n",
      "|text|\n",
      "+----+\n",
      "|   0|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Nan check\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "print(\"Nan checking\")\n",
    "df.select(\n",
    "    [\n",
    "        count(\n",
    "            when(\n",
    "                isnan(c)\n",
    "                | col(c).isNull()\n",
    "                | (col(c) == \"\")\n",
    "                | col(c).contains(\"None\")\n",
    "                | col(c).contains(\"Null\"),\n",
    "                c,\n",
    "            )\n",
    "        ).alias(c)\n",
    "        for c in [\"grade\"]\n",
    "    ]\n",
    ").show()\n",
    "\n",
    "df.select(\n",
    "    [\n",
    "        count(\n",
    "            when(\n",
    "                col(c).contains(\"None\")\n",
    "                | col(c).contains(\"NULL\")\n",
    "                | (col(c) == \"\")\n",
    "                | col(c).isNull()\n",
    "                | isnan(c),\n",
    "                c,\n",
    "            )\n",
    "        ).alias(c)\n",
    "        for c in [\"text\"]\n",
    "    ]\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refs:\n",
    "1. https://github.com/SurajMalpani/NLP-using-Spark/blob/master/nlp-using-pyspark-ml.ipynb\n",
    "2. https://stackoverflow.com/questions/53579444/efficient-text-preprocessing-using-pyspark-clean-tokenize-stopwords-stemming\n",
    "3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```Build steps into pipeline after done```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover, RegexTokenizer, CountVectorizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "df = df.select('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+--------------------------------------------------+\n",
      "|                                              text|                                             words|\n",
      "+--------------------------------------------------+--------------------------------------------------+\n",
      "|Animal Crossing; New Horizons, much like its pr...|[animal, crossing;, new, horizons,, much, like,...|\n",
      "|Know that if you’re overwhelmed with the world,...|[know, that, if, you’re, overwhelmed, with, the...|\n",
      "|With a game this broad and lengthy, there’s mor...|[with, a, game, this, broad, and, lengthy,, the...|\n",
      "|Animal Crossing: New Horizons is everything I h...|[animal, crossing:, new, horizons, is, everythi...|\n",
      "|Above all else, Animal Crossing: New Horizons i...|[above, all, else,, animal, crossing:, new, hor...|\n",
      "+--------------------------------------------------+--------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regex_tokenizer = RegexTokenizer(inputCol='text', outputCol='words', toLowercase=True)\n",
    "raw_words_df = regex_tokenizer.transform(df)\n",
    "raw_words_df.show(5, truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+--------------------------------------------------+\n",
      "|                                             words|                                          filtered|\n",
      "+--------------------------------------------------+--------------------------------------------------+\n",
      "|[animal, crossing;, new, horizons,, much, like,...|[animal, crossing;, new, horizons,, much, like,...|\n",
      "|[know, that, if, you’re, overwhelmed, with, the...|[know, you’re, overwhelmed, world,, stuck, insi...|\n",
      "|[with, a, game, this, broad, and, lengthy,, the...|[game, broad, lengthy,, there’s, discuss, fit, ...|\n",
      "|[animal, crossing:, new, horizons, is, everythi...|[animal, crossing:, new, horizons, everything, ...|\n",
      "|[above, all, else,, animal, crossing:, new, hor...|[else,, animal, crossing:, new, horizons, unbea...|\n",
      "+--------------------------------------------------+--------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stop_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "filtered_df = stop_remover.transform(raw_words_df)\n",
    "filtered_df.select(['words', 'filtered']).show(5, truncate=50)\n",
    "# Note how 'a' is removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+--------------------------------------------------+\n",
      "|                                          filtered|                                           stemmed|\n",
      "+--------------------------------------------------+--------------------------------------------------+\n",
      "|[animal, crossing;, new, horizons,, much, like,...|[anim, crossing;, new, horizons,, much, like, p...|\n",
      "|[know, you’re, overwhelmed, world,, stuck, insi...|[know, you'r, overwhelm, world,, stuck, inside,...|\n",
      "|[game, broad, lengthy,, there’s, discuss, fit, ...|[game, broad, lengthy,, there, discuss, fit, on...|\n",
      "|[animal, crossing:, new, horizons, everything, ...|[anim, crossing:, new, horizon, everyth, hope, ...|\n",
      "|[else,, animal, crossing:, new, horizons, unbea...|[else,, anim, crossing:, new, horizon, unbeat, ...|\n",
      "+--------------------------------------------------+--------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "stemmer_udf = udf(lambda tokens: [stemmer.stem(token) for token in tokens], ArrayType(StringType()))\n",
    "stemmed_df = filtered_df.withColumn(\"stemmed\", stemmer_udf(\"filtered\"))\n",
    "stemmed_df.select('filtered', 'stemmed').show(5, truncate=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+--------------------------------------------------+\n",
      "|                                           stemmed|                                          embedded|\n",
      "+--------------------------------------------------+--------------------------------------------------+\n",
      "|[anim, crossing;, new, horizons,, much, like, p...|(1369,[0,1,9,17,23,28,58,73,137,194,283,330,639...|\n",
      "|[know, you'r, overwhelm, world,, stuck, inside,...|(1369,[1,16,21,48,67,104,128,135,220,230,312,31...|\n",
      "|[game, broad, lengthy,, there, discuss, fit, on...|(1369,[3,6,9,14,15,16,21,30,31,34,36,45,60,94,9...|\n",
      "|[anim, crossing:, new, horizon, everyth, hope, ...|(1369,[0,1,2,4,5,6,29,32,57,64,70,92,95,126,175...|\n",
      "|[else,, anim, crossing:, new, horizon, unbeat, ...|(1369,[0,1,2,3,4,6,7,14,16,18,31,33,35,44,59,72...|\n",
      "+--------------------------------------------------+--------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(inputCol='stemmed', outputCol='embedded')\n",
    "cv_model = count_vectorizer.fit(stemmed_df)\n",
    "\n",
    "embedded_df = cv_model.transform(stemmed_df)\n",
    "embedded_df.select('stemmed', 'embedded').show(5, truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot recognize a pipeline stage of type <class 'nltk.stem.snowball.SnowballStemmer'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mf:\\Projects\\sparkimental\\test-pipeline.ipynb Cell 24\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Projects/sparkimental/test-pipeline.ipynb#X35sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mml\u001b[39;00m \u001b[39mimport\u001b[39;00m Pipeline\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Projects/sparkimental/test-pipeline.ipynb#X35sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m preprocess_pipeline \u001b[39m=\u001b[39m Pipeline(stages\u001b[39m=\u001b[39mstages)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/Projects/sparkimental/test-pipeline.ipynb#X35sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m pipeModel \u001b[39m=\u001b[39m preprocess_pipeline\u001b[39m.\u001b[39;49mfit(df)\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Projects/sparkimental/test-pipeline.ipynb#X35sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m training \u001b[39m=\u001b[39m pipeModel\u001b[39m.\u001b[39mtransform(df)\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Projects/sparkimental/test-pipeline.ipynb#X35sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m training\u001b[39m.\u001b[39mshow(\u001b[39m5\u001b[39m)\n",
      "File \u001b[1;32mf:\\AppSSD\\WorkTools\\Anaconda\\envs\\sparkimental\\lib\\site-packages\\pyspark\\ml\\base.py:161\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[0;32m    160\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 161\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[0;32m    162\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    163\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    164\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params))\n",
      "File \u001b[1;32mf:\\AppSSD\\WorkTools\\Anaconda\\envs\\sparkimental\\lib\\site-packages\\pyspark\\ml\\pipeline.py:101\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[39mfor\u001b[39;00m stage \u001b[39min\u001b[39;00m stages:\n\u001b[0;32m    100\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39misinstance\u001b[39m(stage, Estimator) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(stage, Transformer)):\n\u001b[1;32m--> 101\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    102\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mCannot recognize a pipeline stage of type \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(stage))\n\u001b[0;32m    103\u001b[0m indexOfLastEstimator \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m    104\u001b[0m \u001b[39mfor\u001b[39;00m i, stage \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(stages):\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot recognize a pipeline stage of type <class 'nltk.stem.snowball.SnowballStemmer'>."
     ]
    }
   ],
   "source": [
    "# stages = [\n",
    "#     regex_tokenizer,\n",
    "#     stop_remover,\n",
    "#     stemmer,\n",
    "#     count_vectorizer\n",
    "# ]\n",
    "\n",
    "# from pyspark.ml import Pipeline\n",
    "\n",
    "# preprocess_pipeline = Pipeline(stages=stages)\n",
    "# pipeModel = preprocess_pipeline.fit(df)\n",
    "# training = pipeModel.transform(df)\n",
    "# training.show(5)\n",
    "\n",
    "# # TODO: make snowball stemmer into a custom pipe stage for spark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparkimental",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e4315361361e5693679a33b7caab5ef4ac71969d7d5a82cbcec4fbfd006b2427"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
